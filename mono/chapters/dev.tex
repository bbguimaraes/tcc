\chapter{Desenvolvimento do experimento}

Nesse capítulo é apresentado desenvolvimento do experimento, baseado nos conceitos apresentados nos capítulos anteriores. Em especial, esse capítulo mostra como as ferramentas do capítulo \ref{chap:prop} (\nameref{chap:prop}) foram utilizadas para a comparação dos resultados dos algoritmos apresentados no capítulo \ref{chap:ais} (\nameref{chap:ais}).

\section{Preparação dos dados}

Os dois conjuntos de dados se encontravam em formato de texto, com os atributos separados por espaços e as instâncias separadas pelo caractere nova linha. As listagens \ref{lst:dev_data_ger} e \ref{lst:dev_data_aust} mostram as instâncias dos conjuntos de dados no formato do arquivo original (a primeira coluna representa o número da linha, e não faz parte dos dados)\footnote{Aqui são mostradas apenas as primeiras cinco instâncias dos conjuntos de dados \emph{Cr.Ger} e \emph{Cr.Aust}. Eles possuem 1000 e 650 instâncias, respectivamente}.

\vspace{0.5cm}
\begin{lstlisting}[caption=Formato original dos dados (\emph{Cr.Ger}), label=lst:dev_data_ger]
A11 6 A34 A43 1169 A65 A75 4 A93 A101 4 A121 67 A143 A152 2 A173 1 A192 A201 1
A12 48 A32 A43 5951 A61 A73 2 A92 A101 2 A121 22 A143 A152 1 A173 1 A191 A201 2
A14 12 A34 A46 2096 A61 A74 2 A93 A101 3 A121 49 A143 A152 1 A172 2 A191 A201 1
A11 42 A32 A42 7882 A61 A74 2 A93 A103 4 A122 45 A143 A153 1 A173 2 A191 A201 1
A11 24 A33 A40 4870 A61 A73 3 A93 A101 4 A124 53 A143 A153 2 A173 2 A191 A201 2
\end{lstlisting}
\vspace{0.5cm}

\vspace{0.5cm}
\begin{lstlisting}[caption=Formato original dos dados (\emph{Cr.Aust}), label=lst:dev_data_aust]
1 22.08 11.46 2 4 4 1.585 0 0 0 1 2 100 1213 0
0 22.67 7 2 8 4 0.165 0 0 0 0 2 160 1 0
0 29.58 1.75 1 4 4 1.25 0 0 0 1 2 280 1 0
0 21.67 11.5 1 5 3 0 1 1 11 1 2 0 1 1
1 20.17 8.17 2 6 4 1.96 1 1 14 0 2 60 159 1
\end{lstlisting}
\vspace{0.5cm}

A preparação dos dados para a importação no WEKA consistiu na adição de uma seção de cabeçalho e da formatação dos dados para valores separados por vírgula (seção \ref{sec:prop_arff}), e o resultado é mostrado nas listagens \ref{lst:dev_arff_ger} e \ref{lst:dev_arff_aust} (essas listagens mostram apenas as primeiras cinco instâncias na seção de dados).

\vspace{0.5cm}
\begin{lstlisting}[caption=Arquivo ARFF do \emph{Cr.Ger}, label=lst:dev_arff_ger]
@relation cr.ger

@attribute A1  {A11,A12,A13,A14}
@attribute A2  numeric
@attribute A3  {A30,A31,A32,A33,A34}
@attribute A4  {A40,A41,A42,A43,A44,A45,A46,A47,A48,A49,A410}
@attribute A5  numeric
@attribute A6  {A61,A62,A63,A64,A65}
@attribute A7  {A71,A72,A73,A74,A75}
@attribute A8  numeric
@attribute A9  {A91,A92,A93,A94,A95}
@attribute A10 {A101,A102,A103}
@attribute A11 numeric
@attribute A12 {A121,A122,A123,A124}
@attribute A13 numeric
@attribute A14 {A141,A142,A143}
@attribute A15 {A151,A152,A153}
@attribute A16 numeric
@attribute A17 {A171,A172,A173,A174}
@attribute A18 numeric
@attribute A19 {A191,A192}
@attribute A20 {A201,A202}
@attribute A21 {1,2}

@data
A11,6,A34,A43,1169,A65,A75,4,A93,A101,4,A121,67,A143,A152,2,A173,1,A192,A201,1
A12,48,A32,A43,5951,A61,A73,2,A92,A101,2,A121,22,A143,A152,1,A173,1,A191,A201,2
A14,12,A34,A46,2096,A61,A74,2,A93,A101,3,A121,49,A143,A152,1,A172,2,A191,A201,1
A11,42,A32,A42,7882,A61,A74,2,A93,A103,4,A122,45,A143,A153,1,A173,2,A191,A201,1
A11,24,A33,A40,4870,A61,A73,3,A93,A101,4,A124,53,A143,A153,2,A173,2,A191,A201,2
\end{lstlisting}
\vspace{0.5cm}

\vspace{0.5cm}
\begin{lstlisting}[caption=Arquivo ARFF do \emph{Cr.Aust}, label=lst:dev_arff_aust]
@relation cr.aust

@attribute A1  {0,1}
@attribute A2  numeric
@attribute A3  numeric
@attribute A4  {1,2,3}
@attribute A5  {1,2,3,4,5,6,7,8,9,10,11,12,13,14}
@attribute A6  {1,2,3,4,5,6,7,8,9}
@attribute A7  numeric
@attribute A8  {1,0}
@attribute A9  {1,0}
@attribute A10 numeric
@attribute A11 {1,0}
@attribute A12 {1,2,3}
@attribute A13 numeric
@attribute A14 numeric
@attribute A15 {0,1}

@data
1,22.08,11.46,2,4,4,1.585,0,0,0,1,2,100,1213,0
0,22.67,7,2,8,4,0.165,0,0,0,0,2,160,1,0
0,29.58,1.75,1,4,4,1.25,0,0,0,1,2,280,1,0
0,21.67,11.5,1,5,3,0,1,1,11,1,2,0,1,1
1,20.17,8.17,2,6,4,1.96,1,1,14,0,2,60,159,1
\end{lstlisting}
\vspace{0.5cm}

Com os arquivos no formato ARFF, os conjuntos de dados podem ser importados no WEKA. A figura \ref{fig:dev_weka_arff} mostra o conjunto de dados \emph{Cr.Ger} quando importado. Um conjunto de dados importado pode ser utilizado para qualquer algoritmo que suporte os tipos de atributos contidos nele.

\vspace{0.5cm}
\begin{figure}[h!]
    \centering
    \caption{Conjunto de dados \emph{Cr.Ger importado no WEKA}}
    \label{fig:dev_weka_arff}
    \vspace{0.5cm}
    \includegraphics[width=0.75\textwidth]{img/cr_ger.png}
    \vspace{0.5cm}
\end{figure}
\vspace{0.5cm}

\section{Utilizando o WEKA}

\iffalse experimenter \fi

O WEKA é dividido em dois módulos principais: Explorer e Experimenter. Além disso, o WEKA apresenta duas interfaces principais: linha de comando (\emph{command-line interface}, CLI\nomenclature{CLI}{Command-line Interface}) e gráfica (\emph{graphical user interface}, GUI\nomenclature{GUI}{Graphical User Interface}). A interface gráfica é mais apropriada para exploração e experimentação, e para a apresentação dos dados, algoritmos e resultados. A interface de linha de comando é mais apropriada para automatização de tarefas e integração com outros sistemas, além de consumir menos recursos.

Nesse trabalho foi utilizada a interface por linha de comando, já que a repetição dos testes para cada algoritmo é muito mais fácil do que se fosse utilizada a interface gráfica. Os exemplos de execução são apresentados conforme devem ser digitados em uma interface de linha de comando, em um emulador de terminal, utilizando um \emph{shell} como \emph{sh} ou \emph{bash}.

\subsection{Filtros}

No WEKA, um filtro é um objeto que recebe um conjunto de dados como entrada e produz um conjunto de dados modificado. Esse é um processo comum da Mineração de Dados, chamado de pré-processamento dos dados: adicionar, remover ou alterar atributos, etc.

Um filtro comum, que é utilizado nesse trabalho, é o de criação de partições para o \emph{cross-validation}. Para esse filtro, são passados três argumentos. O argumento \emph{c} indica qual dos atributos é o atributo correspondente à classe, e é representado por um índice, iniciado em 1, conforme a declaração na seção de atributos do arquivo de dados (caso o padrão do WEKA seja usado, ou seja, o atributo de classe seja o último da listagem, pode ser utilizado o valor ``\emph{last}'' como argumento). O argumento \emph{N} indica o número de partições, e o argumento \emph{F} indica a partição selecionada.

Além desses, o argumento \emph{V} pode ser utilizado para gerar o conjunto inverso de seleções, útil para dividir o conjunto em duas partes complementares. Dessa forma, para gerar um conjunto de dados para testes e outro para treinamento, podem ser usados os seguintes comandos\footnote{Nesses exemplos, é usado o redirecionamento de entrada e saída presentes na maioria dos \emph{shells} UNIX. O caractere ``<'' seguido de um nome de arquivo indica que aquele arquivo será usado como entrada para o comando. De maneira semelhante, o caractere ``>'' seguido de um nome de arquivo indica que ele será usado como saída. O WEKA também permite que sejam utilizadas as opções \emph{i} e \emph{o}, respectivamente, para obter os mesmos resultados. No primeiro exemplo, a forma equivalente seria ``\emph{-i dataset.arff -o dataset\_test.arff}''.}:

\begin{lstlisting}[caption=Filtro para geração de partições para \emph{cross-validation}, label=lst:dev_filter]
java weka.filters.supervised.instance.StratifiedRemoveFolds -c last -N 4 -F 1 \
    < dataset.arff > dataset_test.arff
java weka.filters.supervised.instance.StratifiedRemoveFolds -c last -N 4 -F 1 -V \
    < dataset.arff > dataset_train.arff
\end{lstlisting}

\subsection{Execução de um classificador}

A execução de um algoritmo é feita através da classe que implementa o algoritmo no WEKA. Diversas opções podem ser passadas na linha de comando para mudar os parâmetros do algoritmo. Existem algumas opções adicionais para especificar dados adicionais, como os arquivos de dados para treinamento e testes. Além das opções gerais, cada algoritmo pode aceitar diferentes tipo de opções específicas. Como exemplo, para gerar a saída da listagem \ref{lst:prop_weka_out}, foi utilizado o comando da listagem \ref{lst:dev_exec_classifier}.

\vspace{0.5cm}
\begin{lstlisting}[caption=Execução de um classificador, label=lst:dev_exec_classifier]
java weka.classifiers.neural.lvq.Lvq1 -t data/weather.numeric.arff -i
\end{lstlisting}
\vspace{0.5cm}

Como é possível ver, a linha de comando é reproduzida no atributo ``\emph{Scheme}'' na saída. Esse atributo pode ser consultado para executar exatamente o mesmo teste novamente, tornando a reprodução do experimento muito mais fácil. As diversas opções do atributo que não estão presentes na linha de comando são os parâmetros do algoritmo. Como não foram especificados na linha de comando, foram assumidos os valores padrão, que são mostrados na saída. As duas opções que não estão presentes na saída são a opção ``\emph{i}'', que mostra uma saída mais completa e a opção ``\emph{t}'', que indica o arquivo que será utilizado como entrada.

Para executar os algoritmos do pacote de algoritmos imunológicos, é necessário informar à máquina virtual a localização do arquivo que contém o código executável. Esse código é disponibilizado em um arquivo \emph{jar}, um tipo de arquivo específico da linguagem java que é semelhante a um arquivo compactado utilizando os programas \emph{tar} ou \emph{zip}.

Para indicar o arquivo, é utilizada a opção \emph{classpath} da máquina virtual. Essa opção pode ser utilizada tanto para indicar um diretório contendo os arquivos compilados (arquivos \emph{.class}) quanto um arquivo \emph{jar}. Alternativamente, o arquivo \emph{jar} pode ser descompactado e o diretório gerado utilizado. Essa opção pode ser passada na linha de comando ou como uma variável de ambiente. Para executar o algoritmo AIRS, por exemplo, é utilizado qualquer um dos comandos da listagem \ref{lst:dev_weka_airs}. A sintaxe da opção \emph{classpath} é semelhante ao \emph{path} da maioria dos sistemas operacionais, ou seja, uma lista dos caminhos e arquivos separados pelo caractere ``:'' (dois-pontos) em sistemas Unix ou ``;'' (ponto-e-vírgula) em sistemas Windows.

\vspace{0.5cm}
\begin{lstlisting}[caption=Execução de um algoritmo do pacote de algoritmos imunológicos, label=lst:dev_weka_airs]
# Opção na linha de comando.
java -classpath wekaclassalgos.jar weka.classifiers.immune.airs.AIRS1 # parâmetros
# Variável de ambiente.
CLASSPATH=wekaclassalgos.jar
java weka.classifiers.immune.airs.AIRS1 # parâmetros
\end{lstlisting}
\vspace{0.5cm}

Combinando os conceitos apresentados nessa seção, a execução de um teste para um algoritmo imunológico utilizando um dos conjuntos de dados é feita de acordo com a listagem \ref{lst:dev_weka_single}. Aqui, é usado a opção ``\emph{t}'' para especificar o conjunto de dados \emph{Cr.Ger}. Todas as outras opções são parâmetros do filtro (os valores utilizados são os valores padrão para esse algoritmo).

\vspace{0.5cm}
\begin{lstlisting}[caption=Execução de um algoritmo do pacote de algoritmos imunológicos utilizando um dos conjuntos de dados, label=lst:dev_weka_single]
# Variável de ambiente.
CLASSPATH=wekaclassalgos.jar
java weka.classifiers.immune.airs.AIRS1 \
    -S 1 -F 0.2 -C 10.0 -H 2.0 -M 0.1 -R 150.0 -V 0.9 -A -1 -B 1 -E 1 -K 3 \
    -t german.arff
\end{lstlisting}
\vspace{0.5cm}

\subsection{Execução dos classificadores}

Para a execução de um teste, pode ser criado um executor genérico de classificadores, com base na listagem \ref{lst:dev_weka_template}:

\vspace{0.5cm}
\begin{lstlisting}[caption=Execução genérica de um classificador, label=lst:dev_weka_template]
# Variável de ambiente.
java -classpath "$classpath" \
    "$classificador" "$parametros" -t "$conjunto_de_dados" \
    > "$arquivo_resultados"
\end{lstlisting}
\vspace{0.5cm}

Dessa forma, é possível utilizar o mesmo comando para execução, variando apenas os valores das variáveis. Os valores utilizados para cada variável foram:

\begin{enumerate}[a)]
    \item \emph{classpath}: conforme explicado na sessão anterior. Esse valor não muda entre as execuções dos testes, mas é mantido como uma variável para que ele possa ser facilmente alterado caso necessário.
    \item \emph{classificador}: essa variável tem o valor do classificador utilizado no teste.
    \item \emph{parametros}: os parâmetros para o classificador. O valor dessa variável depende do classificador utilizado.
    \item \emph{conjunto\_de\_dados}: o conjunto de dados utilizado para o treinamento e testes.
    \item \emph{arquivo\_resultados}: o arquivo onde a saída da execução será gravada.
\end{enumerate}

Para facilitar a execução de todos os algoritmos, foi utilizado um \emph{script}. O propósito desse programa é ler arquivos de configuração onde são listados os conjuntos de dados utilizados e os classificadores e executar os testes usando todas as combinações de conjunto de dados e classificador. Ou seja, esse programa recebe os classificadores e conjuntos de dados, executa os testes e grava os resultados. Mais concretamente, esse programa dá valores às variáveis descritas acima, da seguinte forma:

\begin{enumerate}[a)]
    \item \emph{classpath}: o arquivo \emph{jar} do WEKA que contém a implementação dos algoritmos imunológicos (\emph{wekaclassalgos.jar}). Esse arquivo também contém os algoritmos do WEKA, então ele é a única dependência necessária para utilizar os classificadores imunológicos e os que já são incluídos no WEKA.
    \item \emph{classificador}: todos os classificadores testados no experimento:
        \begin{enumerate}[a)]
            \item weka.classifiers.immune.airs.AIRS2
            \item weka.classifiers.immune.immunos.Immunos99
            \item weka.classifiers.immune.clonalg.CLONALG
            \item weka.classifiers.functions.MultilayerPerceptron
            \item weka.classifier.functions.SMO
            \item weka.classifiers.meta.AttributeSelectedClassifier
            \item weka.classifiers.trees.J48
            \item weka.classifiers.neural.lvq.Lvq2\_1
        \end{enumerate}
    \item \emph{parâmetros}: os parâmetros são listados junto com cada classificador no arquivo de configuração. \iffalse tabela com os parâmetros de cada classificador \fi
    \item \emph{conjunto\_de\_dados}: o caminho para o arquivo ARFF dos dois conjuntos de dados utilizados.
    \item \emph{arquivo\_resultados}: o nome do arquivo de saída é criado dinamicamente, combinando o nome do classificador, seus parâmetros e o nome do conjunto de dados.
\end{enumerate}

O formato exato do nome do arquivo é mostrado na listagem \ref{lst:dev_output_filename}. O objetivo desse formato é organizar os arquivos de resultados em uma hierarquia simples que facilite a implementação do programa que executa os testes e não gere diretórios com muitos arquivos.

\vspace{0.5cm}
\begin{lstlisting}[caption=Formato do nome do arquivo de resultados, label=lst:dev_output_filename]
# ex: german/AIRS1/S1_F0.2_C10.0_H2.0_M0.1_R150.0_V0.9_A-1_B1_E1_K3.txt
${dataset}/${algoritmo}/${parametros}.txt
\end{lstlisting}
\vspace{0.5cm}

Exemplos dos arquivos de configuração são mostrados nas listagens \ref{lst:dev_classifiers_json} e \ref{lst:dev_datasets_json}. Esses arquivos são lidos no formato JSON (\emph{JavaScript Object Notation}), um formato muito popular em aplicações recentes por ser de fácil leitura e escrita, tanto por humanos quanto por programas.

O primeiro arquivo lista os classificadores e seus parâmetros, o segundo os conjuntos de dados. Eles são gravados em um formato que permite que sejam facilmente transformados nas opções correspondentes na linha de comando.

\vspace{0.5cm}
\begin{lstlisting}[caption=Exemplo de arquivos de configuração de classificadores, label=lst:dev_classifiers_json]
[
    {
        "classifier": "weka.classifiers.trees.J48",
        "params": {
            "C": ["0.25"],
            "M": ["2"]
        }
    }
]
\end{lstlisting}

\begin{lstlisting}[caption=Exemplo de arquivos de configuração de conjuntos de dados, label=lst:dev_datasets_json]
[
    {
        "name": "german",
        "file": "data/statlog_german/german.arff"
    }
]
\end{lstlisting}
\vspace{0.5cm}

\subsection{Seleção de parâmetros no WEKA}

Conforme descrito no capítulo \ref{chap:eval}, o desempenho de um algoritmo depende da escolha dos parâmetros utilizados. Como o processo de escolha de parâmetros é repetitivo e fácil de ser automatizado, existem métodos para escolher os parâmetros que geram o melhor valor para os parâmetros de um modelo para um conjunto de dados específico. Um desses métodos, muito utilizado por sua simplicidade e eficácia (embora não seja tão eficiente), é o \emph{grid search}.

No WEKA, existem duas implementações desse método. Ambos fazem parte de uma categoria denominada \emph{meta-classificadores}\footnote{Esses e outros meta-classificadores que o WEKA disponibiliza podem ser encontrados no pacote \emph{weka.classifiers.meta}.}, ou seja, classificadores que atuam sobre a execução de outros classificadores. Esses dois componentes são \emph{GridSearch} e \emph{CVParameterSelection}, e seu funcionamento é semelhante. O processo para utilizá-los para a escolha de parâmetros é o seguinte:

\begin{enumerate}[a)]
    \item Indicar o meta-classificador utilizado.
    \item Indicar o classificador real que será utilizado.
    \item Listar os parâmetros que serão testados.
    \item Listar a faixa de valores para os parâmetros.
    \item Indicar o conjunto de dados que será utilizado.
    \item Executar o teste.
\end{enumerate}

Com exceção dos itens \emph{a)} e \emph{d)}, o processo é semelhante ã execução de um classificador comum. De fato, o meta-classificador executará o classificador da mesma forma que o WEKA faria. No entanto, ao invés de um única execução com parâmetros fixos, esses dois meta-classificadores fazem diversos testes, utilizando todas as combinações possíveis dos valores listados no item \emph{d)} para os parâmetros.

É importante levar em consideração que, conforme discutido anteriormente, esse processo é de grande utilidade para determinar os melhores valores para os parâmetros do classificador, no entanto, uma óbvia desvantagem é o grande aumento no tempo de execução. O número de combinações possíveis cresce exponencialmente com o número de parâmetros e número de elementos na faixa de valores para esses parâmetros.

Existem formas de amenizar o efeito da explosão combinatória desses métodos. A opção mais popular aproveita o grande potencial de paralelização do processo de execução do classificador com diferentes parâmetros. Como cada execução é completamente independente das outras, e podem ser executadas em paralelo. Na prática, grandes quantidades de unidades de processamento (denominados \emph{clusters}) são utilizados, e conjuntos de combinações de valores são distribuídas para cada um. Dessa forma, é possível aproveitar ao máximo a quantidade de recursos disponíveis. Os resultados dos testes podem então ser combinados em uma única unidade para a análise final.

\subsection{Seleção de parâmetros no experimento}

Devido ao grande número de testes executados utilizando diferentes classificadores e conjuntos de dados, não foram utilizados os métodos oferecidos pelo WEKA. O teste de um meta-classificador é feito em uma só execução e apenas o resultado final é persistido. Todos os resultados intermediários são perdidos assim que o teste é finalizado. Isso faz com que execuções subsequentes de testes que cobrem as mesmas faixas de parâmetros sejam obrigados a executar novamente testes que já foram executados.

Por exemplo, supondo que sejam executados testes conforme a listagem \ref{lst:dev_dry}. Primeiramente, é executado um \emph{grid search} utilizando o algoritmo J48 e apenas o parâmetro ``C'', o limiar de confiança, é otimizado, na faixa [0.1, 0.3]. Na segunda execução, o mesmo classificador é utilizado, porém agora é otimizado também o parâmetro ``M'', o número de instâncias por folha, na faixa [2, 10]. Todos os testes da primeira execução são perdidos assim que ela é finalizada, logo eles devem ser executados novamente na segunda execução.

\vspace{0.5cm}
\begin{lstlisting}[caption=Desperdício de recursos em execuções subsequentes, label=lst:dev_dry]
java weka.classifiers.meta.CVParameterSelection \
    -W weka.classifiers.trees.J48 -P 'C 0.1 0.3 5' \
    -t german.arff
java weka.classifiers.meta.CVParameterSelection \
    -W weka.classifiers.trees.J48 -P 'C 0.1 0.3 5' -P 'M 2 10 9' \
    -t german.arff
\end{lstlisting}
\vspace{0.5cm}

Embora esse exemplo seja simples (o número de testes do primeiro exemplo é 5 e do segundo, 45), o número de testes executados cresce exponencialmente conforme o número de parâmetros testados e a faixa de valores testados aumenta. Esse fator, combinado com um tempo de execução alto para algoritmos e conjunto de dados mais complexos, fazem com que a experimentação seja muito difícil, além de um desperdício extraordinário de recursos, já que os mesmos testes (que sempre têm o mesmo resultado) são executados repetidas vezes.

Por essas razões, foi utilizado um processo diferente para a execução dos testes. Utilizando arquivos no formato da listagem \ref{lst:dev_output_filename}, os testes podem ser executados em sequência, salvando os resultados intermediários. A qualquer momento, os arquivos existentes podem ser consultados para a avaliação dos resultados.

Um efeito colateral dessa implementação é que, como o processo que executa os testes não reexecuta um teste caso o arquivo de resultados exista, a execução dos testes pode ser dividido em diversos processos e até diversas máquinas, com a coordenação feita pelo sistema de arquivos.

\section{Seleção de algoritmos}

Todos os algoritmos utilizados para comparação com os algoritmos imunológicos fazem parte da distribuição padrão do WEKA (a versão utilizada é a 3.6.9 de 25 de janeiro de 2013).

\iffalse screenshot of all algorithms on weka \fi

\begin{enumerate}[a)]
    \item \textbf{Redes neurais artificiais}: um algoritmo que utiliza uma rede de neurônios artificiais inspirados no modelo dos neurônios biológicos do sistema nervoso. É um algoritmo que, como os algoritmos imunológicos, gera um resultado complexo através da colaboração de um grande número de agentes que têm um funcionamento relativamente simples.
        \begin{enumerate}[1.]
            \item \textbf{MultilayerPerceptron}: uma implementação do algoritmo de redes neurais artificiais presente no WEKA, que usa backpropagation, conforme o algoritmo proposto por Paul Werbos \cite{Werbos1974}. Pode ser treinado de forma manual, automática ou uma combinação dos dois.
        \end{enumerate}
    \item \textbf{Árvores de decisão}: um algoritmo supervisionado que monta uma árvore ligando observações sobre os dados à conclusões sobre esses dados. É um tipo de algoritmo preditivo muito usado em sistemas especialistas e aprendizagem de máquina por gerar um modelo que pode ser facilmente analisado e ajustado.
        \begin{enumerate}[1.]
            \item \textbf{ID3}: um algoritmo de árvores de decisão usado em aprendizagem de máquina, desenvolvido por Ross Quinlan \cite{Quinlan1986}. Divide conjunto de atributos dos dados em subconjuntos iterativamente, separando sempre pelo atributo com menor entropia (maior ganho de informação).
            \item \textbf{C4.5}: uma extensão do algoritmo ID3, desenvolvido pelo mesmo autor \cite{Quinlan1993}. Entre as melhorias do algoritmo estão o tratamento mais eficiente de dados contínuos, o tratamento a dados incompletos e a poda da árvore após o treinamento. O WEKA tem uma implementação \emph{open source} desse algoritmo chamado J48.
        \end{enumerate}
    \item \textbf{Learning Vector Quantization (LVQ)}: um algoritmo de classificação supervisionado baseado no \emph{Vector Quantization} (\emph{VQ}), um algoritmo que distribui as instâncias no espaço de estados e iterativamente aproxima as instâncias semelhantes, até que sejam formados \emph{clusters} com as instâncias classificadas \cite{Kohonen1997}.
        \begin{enumerate}[1.]
            \item \textbf{Lvq2\_1} algoritmo presente no pacote de algoritmos de Jason Bronwlee. Uma implementação do algoritmo LVQ onde duas instâncias são analisadas a cada iteração, e só são atualizadas se uma pertence à classe desejada e a outra não e a distância se encontra dentro de uma faixa definida \cite{Brownlee2011w}.
        \end{enumerate}
    \item \textbf{Redes Bayesianas}: um modelo probabilístico desenvolvido usando os modelos derivados do teorema de Bayes \cite{Bayes1763}. O modelo gerado tem a forma de um grafo direcionado acíclico, onde os nodos são variáveis e as arestas as dependências condicionais entre elas \cite{Pearl1988}.
        \begin{enumerate}[1.]
            \item \textbf{Naive Bayes}: um classificador que cria redes bayesianas simples, onde cada atributo do conjunto de dados é analisado independentemente do outro. O resultado é um modelo onde os atributos não influenciam uns aos outros. Por causa dessa limitação, o treinamento desse algoritmo é muito eficiente.
            \item \textbf{Bayes Net}: implementação do algoritmo padrão de redes bayesianas. Permite a escolha a medida de avaliação e o método de busca no espaço de estados utilizados pelo classificador.
        \end{enumerate}
    \item \textbf{Support Vector Machine (SVM)}: esse classificador considera as instâncias como pontos em um espaço de estados e tenta traçar uma linha que separe esses pontos em duas regiões do espaço. Por isso, é considerado um classificador binário linear. Classifica uma nova instância posicionando-a no espaço de estados e identificando em que lado da linha separadora essa instância se encontra \cite{Cortes1995}.
        \begin{enumerate}[1.]
            \item \textbf{Sequential Minimal Optimization (SMO)}: algoritmo iterativo para o problema da otimização de uma máquina de vetor de suporte. A implementação do WEKA é baseada no algoritmo por John Platt \cite{Platt1998}.
        \end{enumerate}
\end{enumerate}

Esses algoritmos podem ser dividos em 5 categorias de algoritmos: árvores de decisão, redes neurais artificiais, redes Bayesianas, \emph{support vector machine} e imunológicos. Na tabela \ref{tbl:dev_algs} são apresentados os mesmo algoritmos, organizados pela categoria a que pertencem.

\vspace{0.5cm}
\begin{table}[h]
    \centering
    \caption{Algoritmos utilizados para comparação}
    \label{tbl:dev_algs}
    \vspace{0.5cm}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Categoria}                         & \textbf{Algoritmos}                    \\
        \hline
        \multirow{2}{*}{Árvores de decisão}        & ID3                                    \\ \cline{2-2}
                                                   & C4.5 (J48)                             \\
        \hline
        \multirow{2}{*}{Redes neurais artificiais} & Multilayer Perceptron                  \\ \cline{2-2}
                                                   & Learning Vector Quantization (Lvq2\_1) \\
        \hline
        \multirow{2}{*}{Redes Bayesianas}          & Naive Bayes                            \\ \cline{2-2}
                                                   & Bayes Net                              \\
        \hline
        Support Vector Machine                     & Sequential Minimal Optimization        \\
        \hline
        \multirow{3}{*}{Imunológicos}              & AIRS                                   \\ \cline{2-2}
                                                   & Immunos                                \\ \cline{2-2}
                                                   & Seleção Clonal (CLONALG)               \\
        \hline
    \end{tabular}
\end{table}
\vspace{0.5cm}

\subsection{ID3}

\begin{enumerate}[a)]
    \item \textbf{Parâmetros}: esse algoritmo não aceita parâmetros.
    \item \textbf{Filtros}: como o ID3 não suporta atributos numéricos, foi necessário discretizar os valores numéricos (seção \ref{sec:prop_weka_filters}).
\end{enumerate}

\iffalse

ID3 does not guarantee an optimal solution, it can get stuck in local optimums. It uses a greedy approach by selecting the best attribute to split the dataset on each iteration. One improvement that can be made on the algorithm can be to use backtracking during the search for the optimal decision tree.

ID3 can overfit to the training data, to avoid overfitting, smaller decision trees should be preferred over larger ones. This algorithm usually produces small trees, but it does not always produce the smallest possible tree.

ID3 is harder to use on continuous data. If the values of any given attribute is continuous, then there are many more places to split the data on this attribute, and searching for the best value to split by can be time consuming.

https://en.wikipedia.org/wiki/ID3_algorithm#Properties

\fi

\subsection{J48}

\begin{enumerate}[a)]
    \item \textbf{Parâmetros}:
        \begin{itemize}
            \iffalse Use unpruned tree. \fi
            \item \textbf{U}: usa uma árvore sem poda.
            \iffalse <pruning confidence>. Set confidence threshold for pruning. (default 0.25) \fi
            \item \textbf{C}: limiar de confiança para poda.
            \iffalse <minimum number of instances>. Set minimum number of instances per leaf. (default 2) \fi
            \item \textbf{M}: número mínimo de instâncias por folha.
            \iffalse Use reduced error pruning.<number of folds>. Set number of folds for reduced error pruning. One fold is used as pruning set. (default 3) \fi
            \item \textbf{R}: reduz o número de instâncias usadas na poda pós-execução.
            \iffalse <number of folds> Set number of folds for reduced error pruning. One fold is used as pruning set. (default 3) \fi
            \item \textbf{N}: número de partições para a poda pós-execução.
            \iffalse Use binary splits only. \fi
            \item \textbf{B}: realiza apenas divisões binárias em atributos numéricos, ao invés de divisões de tamanho arbitrário.
            \iffalse Don't perform subtree raising. \fi
            \item \textbf{S}: desativa a poda baseada em elevação de ramos da árvore.
            \iffalse Do not clean up after the tree has been built. \fi
            \item \textbf{L}: desativa o processo de otimização de memória.
            \iffalse Laplace smoothing for predicted probabilities. \fi
            \item \textbf{A}: aplica o algoritmo de suavização de Laplace às probabilidades da árvore.
            \iffalse <seed>. Seed for random data shuffling (default 1). \fi
            \item \textbf{Q}: semente do gerador de números aleatórios.
        \end{itemize}
\end{enumerate}

\iffalse

The most basic parameter is the tree pruning option. If you decide to employ tree pruning, you will need to consider the options above. Be aware that depending on how the training and test data have been defined that the performance of an unpruned tree may superficially appear better than a pruned one. As described above, this can be a result of overfitting. It is important to experiment with models by intelligently adjusting these parameters. Often, only repeated experiments and familiarity with the data will tease out the best set of options.

http://gautam.lis.illinois.edu/monkmiddleware/public/analytics/decisiontree.html

\fi

\subsection{MultilayerPerceptron}

\begin{enumerate}[a)]
    \item \textbf{Parâmetros}:
        \begin{itemize}
            \iffalse <learning rate>. Learning Rate for the backpropagation algorithm. (Value should be between 0 - 1, Default = 0.3). \fi
            \item \textbf{L}: taxa de aprendizagem.
            \iffalse <momentum>. Momentum Rate for the backpropagation algorithm. (Value should be between 0 - 1, Default = 0.2). \fi
            \item \textbf{M}: taxa de \emph{momentum}.
            \iffalse <number of epochs>. Number of epochs to train through. (Default = 500). \fi
            \item \textbf{N}: número de \emph{epochs} (uma iteração do processo de treinamento e teste).
            \iffalse <percentage size of validation set>. Percentage size of validation set to use to terminate training (if this is non zero it can pre-empt num of epochs. (Value should be between 0 - 100, Default = 0). \fi
            \item \textbf{V}: tamanho do conjunto de validação do treinamento.
            \iffalse <seed>. The value used to seed the random number generator(Value should be >= 0 and and a long, Default = 0). \fi
            \item \textbf{S}: semente do gerador de números aleatórios.
            \iffalse <threshold for number of consequetive errors>. The consequetive number of errors allowed for validation testing before the netwrok terminates. (Value should be > 0, Default = 20). \fi
            \item \textbf{E}: número máximo de erros consecutivos para término da validação da rede.
            \iffalse GUI will be opened. (Use this to bring up a GUI). \fi
            \item \textbf{G}: abre a interface de criação da rede.
            \iffalse Autocreation of the network connections will NOT be done. (This will be ignored if -G is NOT set) \fi
            \item \textbf{A}: não cria as conexões da rede.
            \iffalse A NominalToBinary filter will NOT automatically be used. (Set this to not use a NominalToBinary filter). \fi
            \item \textbf{B}: não aplica o filtro NominalToBinary.
            \iffalse <comma seperated numbers for nodes on each layer>. The hidden layers to be created for the network. (Value should be a list of comma separated Natural numbers or the letters 'a' = (attribs + classes) / 2, 'i' = attribs, 'o' = classes, 't' = attribs .+ classes) for wildcard values, Default = a). \fi
            \item \textbf{H}: parâmetro para configuração das camadas escondidas da rede.
            \iffalse Normalizing a numeric class will NOT be done. (Set this to not normalize the class if it's numeric). \fi
            \item \textbf{C}: desabilita a aplicação de normalização no atributo de classe.
            \iffalse Normalizing the attributes will NOT be done. (Set this to not normalize the attributes). \fi
            \item \textbf{I}: desabilita a aplicação de normalização em todos os atributos.
            \iffalse Reseting the network will NOT be allowed. (Set this to not allow the network to reset). \fi
            \item \textbf{R}: desabilita a reinicialização da rede durante o treinamento.
            \iffalse Learning rate decay will occur. (Set this to cause the learning rate to decay). \fi
            \item \textbf{D}: habilita a diminuição gradual da taxa de aprendizagem.
        \end{itemize}
\end{enumerate}

\subsection{Lvq2\_1}

\begin{enumerate}[a)]
    \item \textbf{Parâmetros}:
        \begin{itemize}
            \iffalse initialisationMode -- Model (codebook vector) initalisation mode (1==Random Training Data Proportional, 2==Random Training Data Even, 3==Random Values In Range, 4==Simple KMeans, 5==Farthest First, 6==K-Nearest Neighbour Even) \fi
            \item \textbf{M}: modo de inicialização dos vetores.
            \iffalse learningFunction -- Learning rate function to use while training, linear is typically better (1==Linear Decay, 2==Inverse, 3==Static) \fi
            \item \textbf{L}: tipo de função para a taxa de aprendizagem.
            \iffalse learningRate -- Initial learning rate value (recommend  0.3 or 0.5) \fi
            \item \textbf{R}: valor inicial da taxa de aprendizagem.
            \iffalse totalCodebookVectors -- Total number of codebook vectors in the model \fi
            \item \textbf{C}: número total de vetores no modelo.
            \iffalse totalTrainingIterations -- Total number of training iterations (recommended 30 to 50 times the number of codebook vectors). \fi
            \item \textbf{I}: número total de iterações.
            \iffalse useVoting -- Use dynamic voting to select the assigned class of each codebook vector, provides automatic handling of misclassified instances. \fi
            \item \textbf{G}: seleção dinâmica da classe de cada instância.
            \iffalse windowSize -- Window size matching codebook vectors must be within (recommend 0.2 or 0.3) \fi
            \item \textbf{W}: limiar de semelhança entre as instâncias comparadas.
        \end{itemize}
\end{enumerate}

\subsection{Naive Bayes}

\begin{enumerate}[a)]
    \item \textbf{Parâmetros}:
        \begin{itemize}
            \iffalse Use kernel density estimator rather than normal distribution for numeric attributes \fi
            \item \textbf{K}: usa Kernel Density Estimator ao invés de distribuição normal para atributos numéricos.
            \iffalse Use supervised discretization to process numeric attributes \fi
            \item \textbf{D}: usa discretização supervisionada para processar atributos numéricos.
        \end{itemize}
\end{enumerate}

\subsection{Bayes Net}

\begin{enumerate}[a)]
    \item \textbf{Parâmetros}:
        \begin{itemize}
            \iffalse Do not use ADTree data structure \fi
            \item \textbf{D}: utiliza ADTree, aumentado a velocidade de treinamento mas consumindo mais recursos.
            \iffalse Search algorithm \fi
            \item \textbf{Q}: método para busca na rede.
            \iffalse Estimator algorithm \fi
            \item \textbf{E}: método para encontrar as tabelas de probabilidade condicional.
        \end{itemize}
\end{enumerate}

\subsection{SMO}

\begin{enumerate}[a)]
    \item \textbf{Parâmetros}:
        \begin{itemize}
            \iffalse The complexity constant C. (default 1) \fi
            \item \textbf{C}: a constante de complexidade das regras de decisão.
            \iffalse Whether to 0=normalize/1=standardize/2=neither. (default 0=normalize) \fi
            \item \textbf{N}: opção para normalizar, uniformizar ou usar os dados oringinais.
            \iffalse The tolerance parameter. (default 1.0e-3) \fi
            \item \textbf{L}: o parâmetro de tolerância.
            \iffalse The epsilon for round-off error. (default 1.0e-12) \fi
            \item \textbf{P}: \emph{epsilon} para erro de arredondamento.
            \iffalse Fit logistic models to SVM outputs. \fi
            \item \textbf{M}: aplica regressão logística aos pesos.
            \iffalse The number of folds for the internal cross-validation. (default -1, use training data) \fi
            \item \textbf{W}: número de partições para o \emph{cross-validation} interno.
            \iffalse The Kernel to use.  (default: weka.classifiers.functions.supportVector.PolyKernel) \fi
            \item \textbf{K}: o \emph{kernel} que será utilizado.
        \end{itemize}
\end{enumerate}

\begin{enumerate}[a)]
    \item \textbf{Parâmetros}:
        \begin{itemize}
            \iffalse antibodyPoolSize -- Antibody pool size (N). The total antibodies maintained in the memory pool and remainder pool. \fi
            \item \textbf{N}: tamanho do \emph{pool} de anticorpos.

            \iffalse clonalFactor -- Clonal factor (beta). Used to scale the number of clones created by the selected best antibodies. \fi
            \item \textbf{B}: taxa de clonagem.

            \iffalse numGenerations -- Total generations. The total number of times that all antigens are exposed to the system. \fi
            \item \textbf{G}: número de gerações.

            \iffalse remainderPoolRatio -- Remainder pool percentage. The percentage of the total antibody pool size allocated for the remainder pool. \fi
            \item \textbf{R}: número de anticorpos mantidos entre cada geração.

            \iffalse selectionPoolSize -- Selection pool size (n). The total number of best antibodies selected for cloning and mutation each iteration. \fi
            \item \textbf{n}: número de anticorpos selecionado para clonagem e mutação a cada geração.

            \iffalse totalReplacement -- Total replacements (d). The total number of antibodies in the remainder pool that are replaced each iteration. Typically 5\%-8\% \fi
            \item \textbf{d}: número de anticorpos descartado e substituído a cada geração.
        \end{itemize}
\end{enumerate}
